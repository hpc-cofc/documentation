---
description: 'News, updates, outages, maintenance periods'
---

# System News and Updates

## September 2019

#### Updates 

* A host of new bioinformatics packages are now installed on the cluster. You can access them using modules from a common area or install the Bioconda package from Anaconda to install them in your home directory. These packages are 
  * bio/vcftools, bio/samtools, bio/ngstools, bio/minimap2, bio/hisat2, bio/bwa, bio/bowtie2, bio/bowtie, bio/bedtools, bio/angsd 
  * see [examples at CofC-HPC GitHub page](https://github.com/hpc-cofc/example-runs/tree/master/10_bio)
* For other newly installed software, please run `module spider`. There should be accompanying example runs at [CofC-HPC GitHub page](https://github.com/hpc-cofc/example-runs/tree/master/10_bio)
* More documentation including YouTube videos are in the works. Please check in later for links

## August 2019

#### Updates 

* Standard version of [AIMALL](http://aim.tkgristmill.com/) is installed to do atoms-in-molecules\(AIM\) analysis on molecular systems starting from quantum mechanical wavefunctions.
* The host name of the login node \(hpc.cofc.edu\) has been aliased as 'openhpc.cofc.edu'
* Documentation on using a Cendio Thinlinc client to get remote desktop access to the cluster is added [here](https://hpc-cofc.gitbook.io/docs/using-the-hpc/quickstart#graphical-user-interface-gui).

#### Maintenance

* A maintenance period will be scheduled at the end of the month to enable database integration to our scheduler and make other changes. 

## July 2019

#### Updates

* Jupyter Notebooks are now available. You can set up your own Anaconda environment to run Python2/3, R and other codes using a web Jupyter Notebook. Please see the [Jupyter Notebooks](using-the-hpc/scheduling-jobs/jupyter-notebooks.md) page 

#### Maintenance

* Network access to the cluster will be intermittent Thursday, July 18, 2019 from 1:30PM to 4:00PM due to a planned firewall upgrade in the datacenter. All your calculations and services that are not dependent on external network access will proceed uninterrupted. Please report any problems to [hpc@cofc.edu](mailto:hpc@cofc.edu).

## June 2019

#### Updates

* The available software has been expanded substantially. Please check the [Software List](using-the-hpc/modules/software.md) or enter `module spider` to see the current list. 

#### Maintenance

* 05/20/19-07/03/19 - Support will be limited due to staff vacations. We will try to accommodate any requests sent to [hpc@cofc.edu](mailto:hpc@cofc.edu).

## May 2019

#### Updates

* R/3.4.2 and R/3.5.2 are added for the GNU7/8-openmpi3 and Intel-openmpi3 stacks
* The available software has been expanded substantially. Please check the Software List or enter `module spider` to see the current list.

#### Outage

* 05/20/19 - a cooling problem in our campus datacenter forced us to power down the cluster for 10 hours. Everything has been restored to its normal state. Please report any problems to [hpc@cofc.edu](mailto:hpc@cofc.edu).

## April 2019

#### Updates

* 04/22/19 - the cluster is open to all campus users. Please feel free to [Request an Account](using-the-hpc/request-access.md)
* 04/05/19 - the cluster is open to a few users for testing. It'll be open to all other users in two weeks if it is deemed ready for production runs. 

## March 2019

### Updates

* 03/07/19 - The HPC cluster is installed by Dell-EMC.
* 03/11/19 - Testing and benchmarking the cluster
* 03/26/19 - HPC software stack installation
* 04/02/19 - Configuration of user environments
* 04/05/19 - Cluster open to a few test users 
* 04/22/19 - Cluster open to all users.





